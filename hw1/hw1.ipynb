{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OSMDXPy8M7C"
   },
   "source": [
    "# HW 1 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zz5Kh9F0xBVf"
   },
   "source": [
    "Welcome to CS 287 HW1. To begin this assignment first turn on the Python 3 and GPU backend for this Colab by clicking `Runtime > Change Runtime Type` above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiugnUMt8M7E"
   },
   "source": [
    "In this homework you will be building several varieties of text classifiers. Text classifiers are not that exciting from an NLP point of view, but they are a great way to get up to speed on the core technologies we will use in this class.\n",
    "\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in PyTorch:\n",
    "\n",
    "1. A naive Bayes unigram classifer (follow Wang and Manning http://www.aclweb.org/anthology/P/P12/P12-2.pdf#page=118: you should only implement Naive Bayes, not the combined classifer with SVM).\n",
    "2. A logistic regression model over word types (you can implement this as $y = \\sigma(\\sum_i W x_i + b)$) \n",
    "3. A continuous bag-of-word neural network with embeddings (similar to CBOW in Mikolov et al https://arxiv.org/pdf/1301.3781.pdf).\n",
    "4. A simple convolutional neural network (any variant of CNN as described in Kim http://aclweb.org/anthology/D/D14/D14-1181.pdf).\n",
    "5. Your own extensions to these models...\n",
    "\n",
    "Consult the papers provided for hyperparameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WjZwPU5OvIzD"
   },
   "source": [
    "Throughout this semester we plan to *beta* test the recently proposed NamedTensor to annotate Tensor's dimensions, because we believe that this makes the code readable and less error-prune. This is an experimental library though, so please let us know if you have any issues. \n",
    "\n",
    "Please see http://nlp.seas.harvard.edu/NamedTensor for more details or https://github.com/harvardnlp/namedtensor to submit a PR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iG0DhOyL8M7E"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "id": "-iEe85198M7F",
    "outputId": "a62e9219-5ac4-4757-e182-0afb277547bd"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchtext opt_einsum\n",
    "!pip install -U git+https://github.com/harvardnlp/namedtensor\n",
    "!pip install -q numpy\n",
    "!pip install -q nltk\n",
    "!pip install -q matplotlib\n",
    "!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHTkeBl-8M7I"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Text text processing library and methods for pretrained word embeddings\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KE37lf0u8M7L"
   },
   "source": [
    "The dataset we will use of this problem is known as the Stanford Sentiment Treebank ( https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf ). It is a variant of a standard sentiment classification task. For simplicity, we will use the most basic form. Classifying a sentence as positive or negative in sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXlrvClg8M7M"
   },
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbZiWCz18M7M"
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = NamedField(names=('seqlen',))\n",
    "\n",
    "# Our labels $y$\n",
    "LABEL = NamedField(sequential=False, names=(), unk_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYweUw-h8M7Q"
   },
   "source": [
    "Next we input our data. Here we will use the standard SST train split, and tell it the fields. Torchtext also gives us the option of using subtrees in the treebank as examples as well. The subtrees can be obtained by passing the option `train_subtrees=True` to splits. Feel free to experiment with using subtrees and report their effect on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQcxFoh88M7R"
   },
   "outputs": [],
   "source": [
    "train, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJOFyfYT8M7V"
   },
   "source": [
    "Let's look at this data. It's still in its original form, we can see that each example consists of a label and the original words.\n",
    "\n",
    "Be sure to double check that examples with neutral labels were filtered out. \n",
    "\n",
    "The length of the training data should be 6920."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "vDgwcxmh8M7W",
    "outputId": "b8601283-f1c8-4126-b22f-25942e3b4734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 6920\n",
      "vars(train[0]) {'text': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Tcw37Xr8M7b"
   },
   "source": [
    "In order to map this data to features, we need to assign an index to each word an label. The function build vocab allows us to do this and provides useful options that we will need in future assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "otfqiXh98M7b",
    "outputId": "c4b5bc0c-b7ca-4ee9-9c07-aefcc872f2b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 16284\n",
      "len(LABEL.vocab) 2\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dBj-iHi8M7f"
   },
   "source": [
    "Finally we are ready to create batches of our training data that can be used for training and validating the model. This function produces 3 iterators that will let us go through the train, val and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = torch.device(\"cpu\")\n",
    "cuda = torch.device(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FV-CSDuX8M7g"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tG2m0rqn19Ze"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XpXAKSbA8M7i"
   },
   "source": [
    "Let's look at a single batch from one of these iterators. The library automatically converts the underlying words into indices. It then produces tensors for batches of x and y. In this case it will consist of the number of words of the longest sentence (with padding) followed by the number of batches. We can use the vocabulary dictionary to convert back from these indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "dcsZVvaG8M7j",
    "outputId": "6aca917f-3e7c-46dd-e287-a1cb9b1b0348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text batch: OrderedDict([('seqlen', 29), ('batch', 10)])\n",
      "Second in batch NamedTensor(\n",
      "\ttensor([ 8941,   169,     5, 11257,  1473,   181,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
      "\t('seqlen',))\n",
      "Converted back to string: Marvelously entertaining and deliriously joyous documentary . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Size of text batch:\", batch.text.shape)\n",
    "example = batch.text.get(\"batch\", 1)\n",
    "print(\"Second in batch\", example)\n",
    "print(\"Converted back to string:\", \" \".join([TEXT.vocab.itos[i] for i in example.tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJxDYWUp8M7m"
   },
   "source": [
    "Similarly it produces a vector for each of the labels in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "k_DcwMRh8M7m",
    "outputId": "31de954f-372f-4e03-c9db-94bb589d8672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of label batch: OrderedDict([('batch', 10)])\n",
      "Second in batch 0\n",
      "Converted back to string: positive\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of label batch:\", batch.label.shape)\n",
    "example = batch.label.get(\"batch\", 1)\n",
    "print(\"Second in batch\", example.item())\n",
    "print(\"Converted back to string:\", LABEL.vocab.itos[example.item()])\n",
    "print(len(batch.text.get(\"batch\", 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgbiW5PP8M7r"
   },
   "source": [
    "Finally the Vocab object can be used to map pretrained word vectors to the indices in the vocabulary. This will be very useful for part 3 and 4 of the problem.  Feel free to experiment with different word vectors and report their effect on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "4ZVfhI6x8M7s",
    "outputId": "9fa56e4b-e1a5-4d9b-ba85-da31a5d9038f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings size  torch.Size([16284, 300])\n",
      "Word embedding of 'follows', first 10 dim  tensor([ 0.3925, -0.4770,  0.1754, -0.0845,  0.1396,  0.3722, -0.0878, -0.2398,\n",
      "         0.0367,  0.2800])\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))\n",
    "\n",
    "print(\"Word embeddings size \", TEXT.vocab.vectors.size())\n",
    "print(\"Word embedding of 'follows', first 10 dim \", TEXT.vocab.vectors[TEXT.vocab.stoi['follows']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4s05JN38M71"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "Using the data given by this iterator, you should construct 4 different torch models that take in batch.text and produce a distribution over labels. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition:  https://www.kaggle.com/c/harvard-cs287-s19-hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kw_PRRx18M72"
   },
   "outputs": [],
   "source": [
    "def test_code(model):\n",
    "    \"All models should be able to be run with following command.\"\n",
    "    upload = []\n",
    "    # Update: for kaggle the bucket iterator needs to have batch_size 10\n",
    "    test_iter = torchtext.data.BucketIterator(test, train=False, batch_size=10)\n",
    "    for batch in test_iter:\n",
    "        # Your prediction data here (don't cheat!)\n",
    "        probs = model(batch.text.to(cuda))\n",
    "        # here we assume that the name for dimension classes is `classes`\n",
    "        argmax = probs.round().int()\n",
    "        upload += argmax.tolist()\n",
    "    with open(\"predictions.txt\", \"w\") as f:\n",
    "        f.write(\"Id,Category\\n\")\n",
    "        for i, u in enumerate(upload):\n",
    "            f.write(str(i) + \",\" + str(u) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiFYDx_58M76"
   },
   "source": [
    "In addition, you should put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/nlp-template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxUB0akyX9iN"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsZt1gUCYGMa"
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNwm7nj7YKyr"
   },
   "outputs": [],
   "source": [
    "def generate_naive_bayes_model(training_data, alpha):\n",
    "  labelCounts = ntorch.ones(2, names=(\"class\")).to(cuda) * 0\n",
    "  vocabCounts = ntorch.ones(len(TEXT.vocab), 2, names=(\"vocab\", \"class\")).to(cuda) * alpha\n",
    "  classes = ntorch.tensor(torch.eye(2), names=(\"class\", \"classIndex\")).to(cuda)\n",
    "  encoding = ntorch.tensor(torch.eye(len(TEXT.vocab)), names=(\"vocab\", \"index\")).to(cuda)\n",
    "  for batch in training_data:\n",
    "    oneHot = encoding.index_select(\"index\", batch.text)\n",
    "    setofwords, _ = oneHot.max(\"seqlen\")\n",
    "    classRep = classes.index_select(\"classIndex\", batch.label)\n",
    "    labelCounts += classRep.sum(\"batch\")\n",
    "    vocabCounts += setofwords.dot(\"batch\", classRep)\n",
    "    \n",
    "  p = vocabCounts.get(\"class\", 1)\n",
    "  q = vocabCounts.get(\"class\", 0)\n",
    "  r = ((p*q.sum())/(q*p.sum())).log()\n",
    "  weight = r\n",
    "  b = np.log(labelCounts.get(\"class\", 1)/labelCounts.get(\"class\", 0))\n",
    "  def naive_bayes(test_batch):\n",
    "    oneHotTest = encoding.index_select(\"index\", test_batch.to(cuda))\n",
    "    setofwords, _ = oneHotTest.max(\"seqlen\")\n",
    "    y = (weight.dot(\"vocab\", setofwords) + b).sigmoid()\n",
    "    return y\n",
    "  \n",
    "  return naive_bayes\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raszJil8wphu"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model):\n",
    "  wrong = 0\n",
    "  total = 0\n",
    "  test_iter = torchtext.data.BucketIterator(test, train=False, batch_size=10)\n",
    "  for batch in test_iter:\n",
    "    probs = model(batch.text.cuda())\n",
    "    predictions = probs.round().int().to(cpu).numpy()\n",
    "    answers = batch.label.to(cpu).numpy()\n",
    "    total += len(predictions)\n",
    "    wrong += np.abs(predictions-answers).sum()\n",
    "  right = total - wrong\n",
    "  return right/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwLTOpFqxkpQ"
   },
   "outputs": [],
   "source": [
    "def naive_bayes_accuracy(alpha):\n",
    "  return get_accuracy(generate_naive_bayes_model(train_iter,alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ3bFfBix5oD"
   },
   "outputs": [],
   "source": [
    "accuracy_tester = np.vectorize(naive_bayes_accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS 287 HW 1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "cs287_assignments_Python_3.6.5",
   "language": "python",
   "name": "cs287_assignments_python_3.6.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
