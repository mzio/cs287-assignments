{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 287 Alt Implementation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Ag0-vjntb8Pt",
        "colab_type": "code",
        "outputId": "cd93bffc-fa38-4b68-e646-6276f1ae75cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchtext spacy opt_einsum\n",
        "!pip install -qU git+https://github.com/harvardnlp/namedtensor\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    17% |█████▌                          | 10kB 16.6MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    51% |████████████████▌               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25h  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for namedtensor (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz (38.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 38.2MB 33.5MB/s \n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "  Running setup.py install for de-core-news-sm ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed de-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "\n",
            "    You can now load the model via spacy.load('de')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AwwdUiBDcKSh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "\n",
        "from namedtensor import ntorch, NamedTensor\n",
        "from namedtensor.text import NamedField\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrtL9COHQFm0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_checkpoint(mod_enc, mod_dec, filename='checkpoint.pth.tar'):\n",
        "    state_dict = {'model_encoder' : mod_enc.state_dict(),\n",
        "                  'model_decoder' : mod_dec.state_dict()}\n",
        "    torch.save(state_dict, filename)\n",
        "    \n",
        "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
        "    state_dict = torch.load(filename)\n",
        "    return state_dict['model_encoder'], state_dict['model_decoder']\n",
        "  \n",
        "def set_parameters(model, sv_model, cuda=True):\n",
        "    for i,p in enumerate(model.parameters()):\n",
        "        p.data = sv_model[list(sv_model)[i]]\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DCo948jncOVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import itertools as it\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import time\n",
        "\n",
        "MAX_LEN = 20\n",
        "MIN_FREQ = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VAmWYW-qcS0D",
        "colab_type": "code",
        "outputId": "fbbcf3cc-8096-467e-da50-30e5e05c6a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "DE = data.Field(tokenize=tokenize_de)\n",
        "\n",
        "# only target needs BOS/EOS:\n",
        "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) \n",
        "\n",
        "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
        "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                         len(vars(x)['trg']) <= MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "de-en.tgz: 100%|██████████| 24.2M/24.2M [00:09<00:00, 2.68MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fe9JyTblcU4U",
        "colab_type": "code",
        "outputId": "5381d456-3b23-49a8-dff7-9ec0afae926c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW3/source_test.txt\n",
        "!head source_test.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 50587  100 50587    0     0   132k      0 --:--:-- --:--:-- --:--:--  132k\n",
            "Als ich in meinen 20ern war , hatte ich meine erste Psychotherapie-Patientin .\n",
            "Ich war Doktorandin und studierte Klinische Psychologie in Berkeley .\n",
            "Sie war eine 26-jährige Frau namens Alex .\n",
            "Und als ich das hörte , war ich erleichtert .\n",
            "Meine Kommilitonin bekam nämlich einen Brandstifter als ersten Patienten .\n",
            "Und ich bekam eine Frau in den 20ern , die über Jungs reden wollte .\n",
            "Das kriege ich hin , dachte ich mir .\n",
            "Aber ich habe es nicht hingekriegt .\n",
            "Arbeit kam später , Heiraten kam später , Kinder kamen später , selbst der Tod kam später .\n",
            "Leute in den 20ern wie Alex und ich hatten nichts als Zeit .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wVtGImwQcW1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
        "\n",
        "pred_set = []\n",
        "for i, line in enumerate(open(\"source_test.txt\"), 1):\n",
        "    words = line.split()# [:-1]\n",
        "    pred_set.append([DE.vocab.stoi[s] for s in words])\n",
        "\n",
        "device = torch.device('cuda')\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits((train, val, test), batch_size=32, device=device,\n",
        "                                                  repeat=False, sort_key=lambda x: len(x.src))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqBFAFwUcYhU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(test_iter))\n",
        "# sent_inspect(batch,4)\n",
        "def sent_inspect(batch, idx=0):\n",
        "    print(\"Source\")\n",
        "    print(' '.join([DE.vocab.itos[w] for w in batch.src.data[:,idx]]))\n",
        "    print(\"Target\")\n",
        "    print(' '.join([EN.vocab.itos[w] for w in batch.trg.data[:,idx]]))\n",
        "# print(DE.vocab.stoi['<pad>'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPKf5huWcaXr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models"
      ]
    },
    {
      "metadata": {
        "id": "s2CZlj0Bcb2e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EmbeddingLM(nn.Module):\n",
        "    def __init__(self, TEXT, dropout=0.0, max_embedding_norm=None, embedding_size=1000):\n",
        "        super(EmbeddingLM, self).__init__()\n",
        "        self.dropout_prob = dropout\n",
        "        self.dropout = nn.Dropout(self.dropout_prob)\n",
        "        \n",
        "        self.vocab_size = len(TEXT.vocab)\n",
        "        self.embedding_dim = embedding_size\n",
        "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim, \n",
        "                                       max_norm=max_embedding_norm)\n",
        "\n",
        "class EncoderLSTM(EmbeddingLM):\n",
        "    def __init__(self, TEXT, hidden_size=500, num_layers=2,\n",
        "                 bidirectional=False, **kwargs):\n",
        "        super(EncoderLSTM, self).__init__(TEXT, **kwargs)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size,\n",
        "                            num_layers=self.num_layers,\n",
        "                            dropout=self.dropout_prob, batch_first=True,\n",
        "                            bidirectional=self.bidirectional)\n",
        "\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        x = self.embeddings(input)  # batch size, sentence len, embedding dim\n",
        "        x = self.dropout(x)\n",
        "        output, hidden = self.lstm(x, hidden)\n",
        "        return output, hidden  # batch, sentence len, hidden size * 1 or 2\n",
        "\n",
        "\n",
        "class DecoderLSTM(EncoderLSTM):\n",
        "    \"\"\"Inherit same architecture as encoder, but reversed\"\"\"\n",
        "    def __init__(self, TEXT, context_size=1, bidirectional_encoder=False, **kwargs):\n",
        "        super(DecoderLSTM, self).__init__(TEXT, **kwargs)\n",
        "        self.context_size = context_size\n",
        "        self.encoder_dirs = 2 if bidirectional_encoder else 1\n",
        "        input_dim = self.context_size * self.num_layers * self.encoder_dirs + 1\n",
        "        self.output = nn.Linear(input_dim * self.hidden_size, self.vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, context):\n",
        "        \"\"\":context: tuple (h, c) of hidden and cell states from t-1 of encoder\"\"\"\n",
        "        x = self.embeddings(input)\n",
        "        x = F.relu(x)\n",
        "        output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        if self.context_size:\n",
        "            context_var = torch.cat(context[:self.context_size])\n",
        "            batch_size = context_var.size(1)\n",
        "            sentence_len = output.size(1)\n",
        "\n",
        "            # Convert this to named version? rn [batch_size, 1, hidden_size * context_size]\n",
        "            context_var = context_var.permute(1, 0, 2).contiguous().view(batch_size, 1, -1)\n",
        "            context_var = context_var.expand(-1, sentence_len, -1)\n",
        "            output = torch.cat((output, context_var), dim=2)\n",
        "\n",
        "        output = self.output(output)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class DecoderAttn(EncoderLSTM):\n",
        "    def __init__(self, TEXT, bidirectional_encoder=False, tie_weights=False,\n",
        "                 linear_encoder=0, **kwargs):\n",
        "        super(DecoderAttn, self).__init__(TEXT, **kwargs)\n",
        "        print('Using final MLP')\n",
        "        self.encoder_dirs = 2 if bidirectional_encoder else 1\n",
        "\n",
        "        dims = self.encoder_dirs  \n",
        "        self.output_decoder = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "        self.output_context = nn.Linear(dims * self.hidden_size, self.vocab_size)\n",
        "\n",
        "        self.linear_encoder = linear_encoder\n",
        "        if self.linear_encoder > 0:\n",
        "            self.attn_linear = nn.Linear(self.encoder_dirs * self.linear_encoder,\n",
        "                                         self.hidden_size)\n",
        "\n",
        "        if tie_weights:\n",
        "            if self.hidden_size != self.embedding_dim:\n",
        "                raise ValueError('Tied weights require hidden size to equal embedding dimension!')\n",
        "            self.output_decoder.weight = self.embeddings.weight\n",
        "        \n",
        "    def forward(self, input, hidden, encoded_output, mask_ix=None):\n",
        "        embedding = self.embeddings(input)\n",
        "        embedding = self.dropout(embedding)\n",
        "        decoder_output, hidden = self.lstm(embedding, hidden)\n",
        "        \n",
        "        # Attention\n",
        "        if self.linear_encoder > 0:\n",
        "            output_linear_encoder = self.attn_linear(encoded_output)\n",
        "        else:\n",
        "            output_linear_encoder = encoded_output\n",
        "\n",
        "        # output_encoder_prm is [batch_size, hidden_size, sentence_len (src)] <- TODO: Named\n",
        "        output_encoder_perm = output_linear_encoder.permute(0, 2, 1)\n",
        "        products = torch.bmm(decoder_output, output_encoder_perm)\n",
        "\n",
        "        # mask_ix is [batch_size, sentence_len_src]\n",
        "        if mask_ix is not None:\n",
        "            mask_ix = Variable(torch.Tensor([np.inf])) * mask_ix\n",
        "            mask_ix[mask_ix != mask_ix] = 0\n",
        "            products = products - torch.unsqueeze(mask_ix, 1)\n",
        "        \n",
        "        product_softmax = F.softmax(products, dim=2)\n",
        "        context = torch.bmm(product_softmax, encoded_output)\n",
        "\n",
        "        output_1 = self.output_decoder(self.dropout(decoder_output))\n",
        "        output_2 = self.output_context(self.dropout(context))\n",
        "        \n",
        "        output = output_1 + output_2\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        \n",
        "        return output, hidden, product_softmax "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RumLO_oPcnZM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Stuff"
      ]
    },
    {
      "metadata": {
        "id": "PgmK45fkchfX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainTestBase(object):\n",
        "    \"\"\"\n",
        "    Parent class for training and evaluation\n",
        "    :models: should be list of [encoder, decoder]\n",
        "    \"\"\"\n",
        "    def __init__(self, models, TEXT_SRC, TEXT_TRG, mask_src=False,\n",
        "                 attention=False, reverse_encoder_input=False, cuda=True):\n",
        "        self.TEXT_SRC = TEXT_SRC\n",
        "        self.TEXT_TRG = TEXT_TRG\n",
        "        self.padding_src = TEXT_SRC.vocab.stoi['<pad>']\n",
        "        self.padding_trg = TEXT_TRG.vocab.stoi['<pad>']\n",
        "\n",
        "        self.models = models\n",
        "        self.mask_src = mask_src\n",
        "        self.use_attention = attention\n",
        "        self.record_attention = False\n",
        "        self.reverse_encoder_input = reverse_encoder_input\n",
        "        self.cuda = cuda and torch.cuda.is_available()\n",
        "        if self.cuda:\n",
        "            print('Using CUDA')\n",
        "\n",
        "    def get_src_and_trg(self, batch):\n",
        "        if self.reverse_encoder_input:\n",
        "            src_data = torch.t(batch.src.data)\n",
        "            ix_rev = torch.LongTensor(np.arange(src_data.size(1) - 1, -1, -1))\n",
        "            src = torch.index_select(torch.t(batch.src.data), dim=1,\n",
        "                                     index=ix_rev)\n",
        "            src = src.contiguous()\n",
        "        else:\n",
        "            src = torch.t(batch.src.data).contiguous()\n",
        "        trg = torch.t(batch.trg.data)\n",
        "        trg_feature = trg[:, :-1].contiguous()\n",
        "        trg_label = trg[:, 1:].contiguous()\n",
        "        return (src, trg_feature, trg_label)\n",
        "\n",
        "    def initial_hidden(self, batch_size, model_ix):\n",
        "        num_directions = 2 if self.models[model_ix].bidirectional else 1\n",
        "        return torch.zeros(self.models[model_ix].num_layers * num_directions, \n",
        "                           batch_size, self.models[model_ix].hidden_size)\n",
        "\n",
        "    def init_hidden(self, batch_size, zeros=True, model_ix=0):\n",
        "        if (self.prev_hidden is not None) and (not zeros):\n",
        "            hidden = self.prev_hidden\n",
        "        else:\n",
        "            hidden = (self.initial_hidden(batch_size, model_ix) for i in range(2))\n",
        "        if self.cuda:\n",
        "            hidden = tuple(h.cuda() for h in hidden)\n",
        "        return tuple(Variable(h) for h in hidden)\n",
        "\n",
        "    def init_model_inputs(self, batch, **kwargs):\n",
        "        if self.cuda:\n",
        "            src, trg_feature, trg_label = tuple(x.cuda() for x in self.get_src_and_trg(batch))\n",
        "        else:\n",
        "            src, trg_feature, trg_label = self.get_src_and_trg(batch)\n",
        "\n",
        "        assert batch.src.size(1) == batch.trg.size(1)\n",
        "        hidden = self.init_hidden(batch.src.size(1), **kwargs)\n",
        "        return (Variable(src), Variable(trg_feature), Variable(trg_label), hidden)\n",
        "\n",
        "    def init_epoch(self):\n",
        "        self.prev_hidden = None\n",
        "\n",
        "    def set_prev_hidden(self, hidden):\n",
        "        if self.models[1].encoder_dirs == 2:\n",
        "            self.prev_hidden = tuple(h[self.models[0].num_layers:, :, :] for h in hidden)\n",
        "        else:\n",
        "            self.prev_hidden = hidden\n",
        "\n",
        "    def get_attn_marsk(self, src):\n",
        "        if self.mask_src:\n",
        "            mask_padding = torch.eq(src, self.padding_src).type(torch.FloatTensor)\n",
        "            mask_padding = mask_padding.cuda() if self.cuda else mask_padding\n",
        "            return mask_padding\n",
        "        else:\n",
        "            return None\n",
        "        \n",
        "    def run_model(self, batch, mode='mean'):\n",
        "        src, trg_feature, trg_label, hidden = self.init_model_inputs(batch, zeros=True, model_ix=0)\n",
        "\n",
        "        encoded_output, encoded_hidden = self.models[0](src, hidden)\n",
        "        self.set_prev_hidden(encoded_hidden)\n",
        "            \n",
        "        if self.use_attention:\n",
        "            mask_padding = self.get_attn_marsk(src)\n",
        "            decoder_output, decoder_hidden, decoder_attn = self.models[1](\n",
        "                trg_feature, self.prev_hidden, encoded_output, mask_padding)\n",
        "            if self.record_attention:\n",
        "                _, pred = torch.topk(decoder_output, k=1, dim=2)\n",
        "                self.attn_log.append((decoder_attn, src, pred.squeeze(), trg_label))\n",
        "        else:\n",
        "            decoder_output, decoder_hidden = self.models[1](\n",
        "                trg_feature, self.prev_hidden, encoded_hidden)\n",
        "            \n",
        "        self.prev_hidden = decoder_hidden\n",
        "        return self.nll_loss(decoder_output, trg_label, mode=mode)\n",
        "\n",
        "    def nll_loss(self, log_probs, output, mode='mean', **kwargs):\n",
        "        batch_size = log_probs.size(0)\n",
        "        typing = torch.cuda.FloatTensor if self.cuda else torch.FloatTensor\n",
        "        sentence_len = torch.sum((output != self.padding_trg).type(typing)) / batch_size\n",
        "\n",
        "        log_probs_ = log_probs.view(-1, log_probs.size(2))\n",
        "        output_ = output.view(-1)\n",
        "        if mode == 'mean':\n",
        "            return F.nll_loss(log_probs_, output_, ignore_index=self.padding_trg, **kwargs) * sentence_len\n",
        "        elif mode == 'sum':\n",
        "            return F.nll_loss(log_probs_, output_, ignore_index=self.padding_trg, size_average=False)\n",
        "\n",
        "            \n",
        "class ModelEval(TrainTestBase):\n",
        "    def __init__(self, models, TEXT_SRC, TEXT_TRG, record_attention=False,\n",
        "                 visualize_freq=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Validation class. Requires matplotlib for the visualization\n",
        "        \"\"\"\n",
        "        super(ModelEval, self).__init__(models, TEXT_SRC, TEXT_TRG,\n",
        "                                           **kwargs)\n",
        "        self.record_attention = record_attention\n",
        "        self.visualize_freq = visualize_freq\n",
        "        \n",
        "    def init_epoch(self):\n",
        "        super(ModelEval, self).init_epoch()\n",
        "        self.attn_log = list()\n",
        "        \n",
        "    def visualize_attn(self, decoder_attn_sample, src_sample, pred_sample,\n",
        "                       trg_label=None, save_name=None):\n",
        "        attn = decoder_attn_sample.cpu().data.numpy()\n",
        "        src_words = np.array(list(map(lambda x: self.TEXT_SRC.vocab.itos[x], \n",
        "                                      src_sample.cpu().data.numpy())))\n",
        "        pred_words = np.array(list(map(lambda x: self.TEXT_TRG.vocab.itos[x], \n",
        "                                       pred_sample.cpu().data.numpy())))\n",
        "        if not trg_label is None:\n",
        "            trg_cpu = trg_label.cpu().data.numpy()\n",
        "            trg_words = np.array(list(map(lambda x : self.TEXT_TRG.vocab.itos[x],\n",
        "                                         trg_cpu)))\n",
        "            pred_words = np.array(['%s (%s)' % (pred_words[i], trg_words[i]) for \\\n",
        "                                   i in range(pred_words.shape[0])])\n",
        "            pad_ix = np.where(trg_words == '<pad>')[0]\n",
        "            if len(pad_ix):\n",
        "                clip_len = pad_ix[0]\n",
        "                trg_words = trg_words[:clip_len]\n",
        "                pred_words = pred_words[:clip_len]\n",
        "                attn = attn[:clip_len, :]\n",
        "        \n",
        "        # Visualizations\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(attn, cmap='gray')\n",
        "        plt.xticks(range(len(src_words)), src_words, rotation='vertical')\n",
        "        plt.yticks(range(len(pred_words)), pred_words)\n",
        "\n",
        "        ax.xaxis.tick_top()\n",
        "\n",
        "        if not save_name is None:\n",
        "            plt.savefig(save_name)\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self, test_iter, num_iter=None):\n",
        "        start_time = time.time()\n",
        "        for model in self.models:\n",
        "            model.eval()\n",
        "\n",
        "        nll_sum = 0\n",
        "        nll_count = 0\n",
        "\n",
        "        self.init_epoch()\n",
        "        test_iter.init_epoch()\n",
        "\n",
        "        for i, batch in enumerate(test_iter):\n",
        "            nll_count += batch.trg.data.numel()\n",
        "            loss = self.run_model(batch, mode='sum')\n",
        "            nll_sum += loss.data.item()\n",
        "            \n",
        "            if self.visualize_freq and i % self.visualize_freq == 0:\n",
        "                sample = self.attn_log[-1]\n",
        "                self.visualize_attn(sample[0][0], sample[1][0], sample[2][0])\n",
        "            if not num_iter is None and i > num_iter:\n",
        "                break\n",
        "                        \n",
        "        for model in self.models:\n",
        "            model.train()\n",
        "        \n",
        "        print('Validation time: %f seconds' % (time.time() - start_time))\n",
        "        return np.exp(nll_sum / nll_count)\n",
        "    \n",
        "    # Performs beam search\n",
        "    def beam_search_predict(self, sentence, ref_beam, ref_vocab, \n",
        "                            beam_size=100, pred_len=3, pred_num=None,\n",
        "                            ignore_eos=False, translate=False):\n",
        "        \"\"\"Beam search implementation for selecting viable translations\"\"\"\n",
        "        if pred_num is None:\n",
        "            pred_num = beam_size\n",
        "        \n",
        "        # [sentence_len]\n",
        "        tensor_sentence = torch.LongTensor(sentence)\n",
        "        if self.reverse_encoder_input:\n",
        "            ix_rev = torch.LongTensor(np.arange(tensor_sentence.size(0) - 1, -1, -1))\n",
        "            tensor_sentence = torch.index_select(tensor_sentence, dim=0,\n",
        "                                          index=ix_rev)\n",
        "        if self.cuda:\n",
        "            tensor_sentence = tensor_sentence.cuda()   \n",
        "\n",
        "        src = Variable(tensor_sentence.view(1, -1).expand(beam_size, -1))\n",
        "        hidden = self.init_hidden(beam_size, zeros=True)\n",
        "        \n",
        "        # For attention, will use encoder_output (not otherwise)\n",
        "        encoder_output, encoder_hidden = self.models[0](src, hidden)\n",
        "        self.set_prev_hidden(encoder_hidden)\n",
        "        \n",
        "        sos_token = self.TEXT_TRG.vocab.stoi['<s>']  # Start with SOS  \n",
        "        self.current_beams = (sos_token * torch.ones(beam_size, 1)).type(torch.LongTensor)\n",
        "        self.current_beam_vals = torch.zeros(beam_size, 1).type(torch.FloatTensor)\n",
        "        if self.cuda:\n",
        "            self.current_beams = self.current_beams.cuda()\n",
        "            self.current_beam_vals = self.current_beam_vals.cuda()\n",
        "        self.current_beams = Variable(self.current_beams)\n",
        "        self.current_beam_vals = Variable(self.current_beam_vals)\n",
        "\n",
        "        if translate:\n",
        "            final_preds = list()\n",
        "\n",
        "        for i in range(pred_len):\n",
        "            if translate:\n",
        "                (ref_beam, ref_vocab) = self.create_ref_arrays(self.current_beams.size(0))\n",
        "\n",
        "            current_sentence = self.current_beams[:, i:i+1]\n",
        "            if self.use_attention:\n",
        "                mask_padding = self.get_attn_marsk(src)\n",
        "\n",
        "                decoder_output, decoder_hidden, decoder_attn = self.models[1](\n",
        "                    current_sentence, self.prev_hidden, encoder_output[:self.current_beams.size(0)], mask_padding)\n",
        "                if self.record_attention:\n",
        "                    _, pred = torch.topk(decoder_output, k=1, dim=2)\n",
        "                    self.attn_log.append((decoder_attn, src, pred.squeeze(), None))\n",
        "            else:\n",
        "                decoder_output, decoder_hidden = self.models[1](\n",
        "                    current_sentence, self.prev_hidden, encoder_hidden)\n",
        "            self.prev_hidden = decoder_hidden\n",
        "            \n",
        "            decoder_output = decoder_output.squeeze()\n",
        "\n",
        "            if ignore_eos:  # EOS tokens  \n",
        "                eos_token = self.TEXT_TRG.vocab.stoi['</s>']\n",
        "                decoder_output[:, eos_token] = -np.inf\n",
        "\n",
        "            decoder_output = decoder_output + self.current_beam_vals\n",
        "\n",
        "            if i == 0:\n",
        "                decoder_output = decoder_output[0, :]\n",
        "            else:\n",
        "                decoder_output = decoder_output.view(-1)\n",
        "                \n",
        "            topk_dec, topk_ix = torch.topk(decoder_output, k=beam_size)\n",
        "            prev_ix = torch.index_select(ref_beam, dim=0, index=topk_ix)\n",
        "            prevs = torch.index_select(self.current_beams, dim=0, index=prev_ix)\n",
        "\n",
        "            # Update hidden to reflect previous sentences picked\n",
        "            self.prev_hidden = tuple(torch.index_select(\n",
        "                    self.prev_hidden[j], dim=1, index=prev_ix) for j in range(len(self.prev_hidden)))\n",
        "            \n",
        "            # Update current baem values  \n",
        "            self.current_beam_vals = topk_dec.view(-1, 1)\n",
        "            nexts = torch.index_select(ref_vocab, dim=0, index=topk_ix).view(-1, 1)\n",
        "            self.current_beams = torch.cat((prevs, nexts), dim=1)\n",
        "\n",
        "            if translate:\n",
        "                sentences_ = list()\n",
        "                eos_token = self.TEXT_TRG.vocab.stoi['</s>']\n",
        "                for i in range(self.current_beams.size(0)):\n",
        "                    if self.current_beams[i,-1].data.item() == eos_token:\n",
        "                        final_preds.append(self.current_beams[i,:])\n",
        "                    else:\n",
        "                        sentences_.append(i)\n",
        "                keep_ix = Variable(torch.LongTensor(sentences_))\n",
        "                keep_ix = keep_ix.cuda() if self.cuda else keep_ix\n",
        "\n",
        "                # Now reselect\n",
        "                if len(sentences_) > 100 or len(keep_ix) == 0:\n",
        "                    return final_preds           \n",
        "                self.current_beams = torch.index_select(self.current_beams, 0, keep_ix)\n",
        "                self.current_beam_vals = torch.index_select(self.current_beam_vals, 0, keep_ix)\n",
        "                self.prev_hidden = tuple(torch.index_select(self.prev_hidden[j], 1, keep_ix) \n",
        "                                         for j in range(len(self.prev_hidden)))\n",
        "\n",
        "        if translate:\n",
        "            return final_preds\n",
        "        return self.current_beams\n",
        "    \n",
        "    def escape(self, l):\n",
        "        return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")\n",
        "\n",
        "    def create_ref_arrays(self, beam_size):\n",
        "        # Keep track of ix for expanding beams and vocab\n",
        "        trg_vocab_size = len(self.TEXT_TRG.vocab)\n",
        "        ref_beam = torch.LongTensor(np.arange(beam_size)).view(-1, 1).expand(-1, trg_vocab_size)\n",
        "        ref_beam = ref_beam.contiguous().view(-1)\n",
        "        ref_beam = ref_beam.cuda() if self.cuda else ref_beam\n",
        "        ref_beam = Variable(ref_beam)\n",
        "        \n",
        "        ref_vocab = torch.LongTensor(np.arange(trg_vocab_size)).view(1, -1).expand(beam_size, -1)\n",
        "        ref_vocab = ref_vocab.contiguous().view(-1)\n",
        "        ref_vocab = ref_vocab.cuda() if self.cuda else ref_vocab\n",
        "        ref_vocab = Variable(ref_vocab)\n",
        "        return (ref_beam, ref_vocab)\n",
        "    \n",
        "    def create_text(self, word_ids, src, is_variable=False):\n",
        "        \"\"\":src: True or False specifying what language\"\"\"\n",
        "        TEXT = self.TEXT_SRC if src else self.TEXT_TRG\n",
        "        if is_variable:\n",
        "            words = [TEXT.vocab.itos[word_ids[k].data.item()] for k in range(1, len(word_ids))]\n",
        "        else:\n",
        "            words = [TEXT.vocab.itos[word_ids[k]] for k in range(1, len(word_ids))]\n",
        "        return ' '.join(words)\n",
        "        \n",
        "    \n",
        "    def view_predictions(self, batch_src, batch_trg, batch_preds):\n",
        "        for i,sent in enumerate(batch_src):\n",
        "            if i > 10:\n",
        "                break\n",
        "            words_src = self.create_text(sent, True)\n",
        "            words_trg = self.create_text(batch_trg[i], False)\n",
        "            words_pred_list = list()\n",
        "            for j in range(len(batch_preds[i])):\n",
        "                words_pred_list.append(self.create_text(batch_preds[i][j], False,\n",
        "                                                       is_variable=True))\n",
        "            for w in words_pred_list:\n",
        "                print(w)\n",
        "            \n",
        "        \n",
        "    def predict(self, test_set, fname='predictions.txt', num_cands=100, pred_len=3,\n",
        "                beam_size=100, ignore_eos=False, translate=False):\n",
        "        start_time = time.time()\n",
        "        for model in self.models:\n",
        "            model.eval()\n",
        "\n",
        "        (ref_beam, ref_vocab) = self.create_ref_arrays(beam_size)\n",
        "            \n",
        "        self.init_epoch()\n",
        "        predictions = []\n",
        "        for i, sentence in enumerate(test_set):\n",
        "            # [pred_num, pred_len] tensor\n",
        "            translations = self.beam_search_predict(sentence, ref_beam=ref_beam, \n",
        "                                                    ref_vocab=ref_vocab, pred_len=pred_len,\n",
        "                                                    beam_size=beam_size, ignore_eos=ignore_eos,\n",
        "                                                    translate=translate)\n",
        "            predictions.append(translations)            \n",
        "        if translate:\n",
        "            return predictions\n",
        "            \n",
        "        print('Writing predictions to %s' % fname)\n",
        "        with open(fname, 'w') as fout:\n",
        "            print('id,Predicted', file=fout)\n",
        "            for i, preds in enumerate(predictions):\n",
        "                candidates = []\n",
        "                for j in range(num_cands):\n",
        "                    words = [self.TEXT_TRG.vocab.itos[preds[j,k].data.item()] for k in range(1, pred_len + 1)]\n",
        "                    sent = '|'.join(self.escape(l) for l in words)\n",
        "                    candidates.append(sent)\n",
        "                print('%d,%s' % (i, ' '.join(candidates)), file=fout)\n",
        "        print('Computing predictions took %f seconds' % (time.time() - start_time))\n",
        "        \n",
        "        # Wrap model.eavl\n",
        "        for model in self.models:\n",
        "            model.train()\n",
        "            \n",
        "\n",
        "    \n",
        "class ModelTrain(TrainTestBase):\n",
        "    def __init__(self, models, TEXT_SRC, TEXT_TRG, lr=0.1, optimizer=optim.SGD, \n",
        "                 lrn_decay=None, lrn_decay_force=np.inf, lrn_decay_rate=0.1,\n",
        "                 clip_norm=10, **kwargs):\n",
        "        '''\n",
        "        Class to train models.  \n",
        "        :lr_decay_type: type of learning rate decay, pick 'adaptive' or 'linear'\n",
        "        '''\n",
        "        super(NMTTrainer, self).__init__(models, TEXT_SRC, TEXT_TRG, **kwargs)\n",
        "\n",
        "        self.base_lr = lr\n",
        "        self.optimizer_type = optimizer\n",
        "        self.init_optimizers()  # Optimizer for each model\n",
        "\n",
        "        # Learning rate decay\n",
        "        self.lr_decay_type = lrn_decay\n",
        "        self.lr_decay_force = lrn_decay_force\n",
        "\n",
        "        if self.lr_decay_type is None or self.lr_decay_type == 'adaptive':\n",
        "            self.lr_lambda = lambda i : 1\n",
        "        elif self.lr_decay_type == 'linear':\n",
        "            decay_rate = lrn_decay_rate\n",
        "            self.lr_lambda = lambda i : 1 / (1 + (i - 6) * decay_rate) if i > 6 else 1\n",
        "        \n",
        "        self.schedulers = [optim.lr_scheduler.LambdaLR(optimizer,\n",
        "            self.lr_lambda) for optimizer in self.optimizers]\n",
        "\n",
        "        self.clip_norm = clip_norm\n",
        "        self.restart_logs()\n",
        "        if self.cuda:\n",
        "            for model in self.models:\n",
        "                model.cuda()\n",
        "                \n",
        "    def init_optimizers(self):\n",
        "        self.optimizers = [self.optimizer_type(filter(lambda p : p.requires_grad,\n",
        "                                                      model.parameters()),\n",
        "                                               lr = self.base_lr) for \\\n",
        "                           model in self.models]\n",
        "\n",
        "    def restart_logs(self):\n",
        "        self.training_losses = list()\n",
        "        self.training_norms = list()\n",
        "        self.val_performance = list()\n",
        "\n",
        "    def get_loss_data(self, loss):\n",
        "        if self.cuda:\n",
        "            return loss.data.cpu().numpy()\n",
        "        else:\n",
        "            return loss.data.numpy()[0]\n",
        "\n",
        "    def record_updates(self, loss, norm):\n",
        "        self.training_norms.append(norm)\n",
        "        self.training_losses.append(loss)\n",
        "\n",
        "    def clip_norms(self):\n",
        "        if self.clip_norm > 0:\n",
        "            parameters = tuple()\n",
        "            for model in self.models:\n",
        "                parameters += tuple(model.parameters())\n",
        "                \n",
        "            norm = nn.utils.clip_grad_norm(parameters, self.clip_norm)\n",
        "        else:\n",
        "            norm = -1\n",
        "        return norm\n",
        "\n",
        "    def train_batch(self, batch, **kwargs):\n",
        "        for model in self.models:\n",
        "            model.zero_grad()\n",
        "        loss = self.run_model(batch)\n",
        "        loss.backward()\n",
        "\n",
        "        norm = self.clip_norms()\n",
        "\n",
        "        loss_data = self.get_loss_data(loss)\n",
        "\n",
        "        if kwargs.get('verbose', False):\n",
        "            self.record_updates(loss_data, norm)\n",
        "\n",
        "        for optimizer in self.optimizers:\n",
        "            optimizer.step()\n",
        "\n",
        "        return loss_data, norm\n",
        "\n",
        "    def init_parameters(self):\n",
        "        for model in self.models:\n",
        "            for p in model.parameters():\n",
        "                p.data.uniform_(-0.05, 0.05)\n",
        "\n",
        "    def train(self, train_iter, eval_=None, val_iter=None,\n",
        "              save_model_fname=None, init_parameters=True, **kwargs):\n",
        "        self.restart_logs()\n",
        "        start_time = time.time()\n",
        "        print(\"Initializing parameters status: \", init_parameters)\n",
        "        if init_parameters:\n",
        "            self.init_parameters()\n",
        "\n",
        "        train_iter.init_epoch()\n",
        "\n",
        "        for epoch in range(kwargs.get('num_iter', 100)):\n",
        "            self.init_epoch()\n",
        "            for model in self.models:\n",
        "                model.train()\n",
        "                \n",
        "            # Learning rate decay?\n",
        "            if self.lr_decay_type == 'adaptive':\n",
        "                if (epoch > 2 and self.val_performance[-1] > self.val_performance[-2]) or (epoch >= self.lr_decay_force):\n",
        "                    self.base_lr = self.base_lr / 2\n",
        "                    self.init_optimizers()  \n",
        "                    print('Decay LR to %f' % self.base_lr)\n",
        "            else:\n",
        "                for scheduler in self.schedulers:\n",
        "                    scheduler.step()\n",
        "\n",
        "            train_iter = iter(train_iter)\n",
        "\n",
        "            for batch in train_iter:\n",
        "                result_loss, result_norm = self.train_batch(batch, **kwargs)\n",
        "\n",
        "            if epoch % kwargs.get('skip_iter', 1) == 0:\n",
        "                if not kwargs.get('verbose', False):\n",
        "                    self.record_updates(result_loss, result_norm)\n",
        "\n",
        "            print('Epoch %d, loss: %f, norm: %f, elapsed: %f, lr: %f' \\\n",
        "                  % (epoch, np.mean(self.training_losses[-10:]),\n",
        "                     np.mean(self.training_norms[-10:]),\n",
        "                     time.time() - start_time, self.base_lr)) \n",
        "                    \n",
        "            if (eval_ is not None) and (val_iter is not None):\n",
        "                self.val_performance.append(eval_.evaluate(val_iter))\n",
        "                print('Validation set metric: %f' % \\\n",
        "                      self.val_performance[-1])\n",
        "\n",
        "            if save_model_fname is not None:\n",
        "                path_name = save_model_fname + '.epoch_%d.ckpt.tar' % epoch\n",
        "                print('Saving model')\n",
        "                save_checkpoint(self.models[0], self.models[1], path_name)\n",
        "\n",
        "        if len(self.val_performance) >= 1:  # Check for final submission, but probably will stop before this\n",
        "            print('FINAL VAL PERF', self.val_performance[-1])\n",
        "            return self.val_performance[-1]\n",
        "        return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k-tJJdp3Z6fe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Call these to train new model  \n",
        "Better to load saved model below"
      ]
    },
    {
      "metadata": {
        "id": "5mPud26iuGwB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# device = torch.device('cuda')\n",
        "# train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=32, device=device,\n",
        "#                                                   repeat=False, sort_key=lambda x: len(x.src))\n",
        "# bs_encoder = Encod`erLSTM(DE, hidden_size=500, num_layers=4, embedding_size=500)\n",
        "# at_decoder = DecoderAttn(EN, hidden_size=500, num_layers=4, embedding_size=500)\n",
        "# trainer = ModelTrain([bs_encoder, bs_decoder], DE, EN, lr=0.7, \n",
        "#                      lr_decay_type='adaptive', reverse_encoder_input=False)\n",
        "# evaluator = ModelEval([bs_encoder, bs_decoder], DE, EN, reverse_encoder_input=False)\n",
        "# trainer.train(train_iter, val_iter=val_iter,eval_=evaluator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vWgxCYs2JcSN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=32, device=device,\n",
        "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
        "bs_encoder = EncoderLSTM(DE, hidden_size=650, num_layers=4, embedding_size=650, \n",
        "                         bidirectional=True, dropout=0.35)\n",
        "at_decoder = DecoderAttn(EN, hidden_size=650, num_layers=4, embedding_size=650, \n",
        "                         dropout=0.35, tie_weights=True, linear_encoder=650, \n",
        "                         bidirectional_encoder=True)\n",
        "trainer = ModelTrain([bs_encoder, at_decoder], DE, EN, lr=1.2, \n",
        "                     lr_decay_type='adaptive', attention=True,\n",
        "                     clip_norm=5)\n",
        "evaluator = ModelEval([bs_encoder, at_decoder], DE, EN, attention=True,\n",
        "                        record_attention=False)\n",
        "trainer.train(train_iter, verbose=True, eval_=evaluator, val_iter=val_iter,\n",
        "              save_model_fname='attn', num_iter=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-Hm5qrgZ973",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load saved model for prediction"
      ]
    },
    {
      "metadata": {
        "id": "YVIup9EXLFSx",
        "colab_type": "code",
        "outputId": "8c8b7fe4-05d6-4330-dd78-416848aa5704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "ld_enc, ld_dec = load_checkpoint('attn.epoch_10.ckpt.tar')\n",
        "ld_encoder = EncoderLSTM(DE, hidden_size=650, num_layers=4, embedding_size=650, \n",
        "                         bidirectional=True, dropout=0.35)\n",
        "ld_decoder = DecoderAttn(EN, hidden_size=650, num_layers=4, embedding_size=650, \n",
        "                         dropout=0.35, tie_weights=False, linear_encoder=650, \n",
        "                         bidirectional_encoder=True)\n",
        "set_parameters(ld_encoder, ld_enc)\n",
        "set_parameters(ld_decoder, ld_dec)\n",
        "evaluator = ModelEval([ld_encoder, ld_decoder], DE, EN, attention=True,\n",
        "                        record_attention=False)\n",
        "evaluator.predict(pred_set, fname='predictions_100.txt', beam_size=100, ignore_eos=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using final MLP\n",
            "Using CUDA\n",
            "Writing predictions to predictions_100.txt\n",
            "Computing predictions took 101.902663 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o98nPOcyVC5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}